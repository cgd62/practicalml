---
title: "Practical Machine Learning Course Project"
author: "Greg Doherty"
date: "September 20th, 2014"
output:
  html_document:
    toc: true
    theme: journal
---

# Using Biometric Data To Determine "Goodness" of Exercise

This project uses the data set of biometrics from the paper cited below to create a prediction model that provides feedback on whether an exercise has been done in a proper way.

Citation:
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf

# Initialization & Data Load

For initialization we load the caret library and the doMC library to make use of caret's parallel mode.  Will try a random forest and a support vector machine to compare and choose the best, so load the randomForest and kernlab libraries, and e1071 for its utility functions. caret would load these automatically but I wanted to keep my subsequent output cleaner below.

Using the doMC library, the number of cores to use for parallel processing is set to 4.  caret by default makes use of parallel functionality if it is provided.  This helps reduce elapsed time to build models.

The training and test data sets are loaded, and read.csv() is allowed to convert appropriate values to factors.

```{r setup}
library(caret)
library(doMC)
library(randomForest)
library(kernlab)
library(e1071)

registerDoMC(cores = 4)

# load data sets, converting non-numerics to factors
training <- read.csv("pml-training.csv",as.is=F)
test <- read.csv("pml-testing.csv",as.is=F)
```

# Data Evaluation, Cleaning & Predictor Selection

The data set has `r ncol(training)-1` attributes, and `r nrow(training)` examples.  The last attribute, _classe_, provides the correct result for each example.

Looking over the data set, there is a lot of sparseness and NA's, so a little work needs to be done on picking good predictors to build a model with (feature selection).

## Selecting by rationale
Looking at the first few columns:
``` {r peek}
str(training,list.len=5)
```
It is reasonable to assume that record sequence number N and user_name will not contribute.  The timestamps and converted dates could be used if creating a time series predictor window set were going to be used, but it is not necessary.  Preliminary analysis found great models without having to use time sequence as a factor, except I do leave num_window and factor new_window as potential proxies for sequence/time, though new_window is removed by the skewness step below.

``` {r removerat}
removeCols <- c( 1:5 )           # remove N, user_name, timestamp(s), converted date
```

## Selecting by decent percentage of values != NA
Columns that are more than half NA are not very useful for prediction.

``` {r removeNA}
NA_threshold <- 0.50
nTrain <- nrow(training)

i <- 1
while(i < ncol(training)) {
  nNA <- sum(is.na(training[,i]))
  if((nNA/nTrain) >= NA_threshold) {
     removeCols <- c(removeCols, i)
  }
  i <- i + 1
}

training <- training[,-removeCols]
test <- test[,-removeCols]
```
## Selecting by reasonable skewness
Columns that have extremely high skew are also not very useful for prediction.

``` {r removeskew}
i <- 1
removeCols <- c( )
while(i < ncol(training)) {
  skew <- abs(skewness(as.numeric(training[,i]),na.rm=T))
  if(skew > 6) {
     removeCols <- c(removeCols, i)
  }
  i <- i + 1
}

training <- training[,-removeCols]
test <- test[,-removeCols]
```
## These Are The Attributes Selected As Predictors

``` {r featsel}
str(training)
```

# Create Training and Cross-Validation data sets
Set aside 25% of the training data for cross-validation.
``` {r traintest}
inTrain <- createDataPartition(y=training$classe, p=0.75, list=FALSE)

train <- training[inTrain,]
crossval <- training[-inTrain,]
```

# Train Two Kinds of Models
Model/method selection in caret is a data mining project in and of itself since there are **so many** methods supported.  I picked two that I knew from the literature were very good in general, random forests and support vector machines.  For the support vector machine I selected a radial basis function as the kernel trick.

For development purposes the code below supports saving trained models and restoring from a file, if the file exists.  This makes iterating during development possible as the models take over an hour to train.

(For the record, I also tried neural nets, but found them slow to train, limited by number of hidden nodes (roughly < 20) and I could not figure out how to pass down the 'size' parameter properly.  The accuracy for the neural net I was able to train was 43%, so I eliminated neural nets from final use in this project.) 

``` {r trainmodels}
# load saved models if they exist, otherwise, we have to train them.  remove file for final run.

if(file.exists("mymodels.sav")) {
  load("mymodels.sav")
} else {
  fitControl <- trainControl(method="repeatedcv",repeats=3)
  
  RFmodelFit <- train(classe ~ .,data=train,method="rf",trControl=fitControl)
  
  SVMmodelFit <- train(classe ~ .,data=train,method="svmRadial",trControl = fitControl)

  # save these models to disk as they take a lot of time to train
  save(list = c("RFmodelFit","SVMmodelFit"),file="mymodels.sav")
}
```

# Evaluate And Compare Models
Now evaluate models using the cross-validation data set.

This allows out of sample error to be estimated. See the 95% confidence interval in the confusion matrix for each model.

To choose best goodness of fit use Kappa metric.

## Random Forest Model Summary
```{r summrf}
RFmodelFit
plot(RFmodelFit)
RFpred <- predict(RFmodelFit,newdata=crossval)
RFcv <- confusionMatrix(RFpred,crossval$classe)
RFcv
```

## Support Vector Machine Model Summary
``` {r summsvm}
SVMmodelFit
plot(SVMmodelFit)
SVMpred <- predict(SVMmodelFit,newdata=crossval)
SVMcv <- confusionMatrix(SVMpred,crossval$classe)
SVMcv
```

## Use the Kappa value to select the best model to make final predictions with
``` {r select}
#cv$overall[2] is the Kappa value for the model from the confusion matrix.  Pick the best model based on Kappa.

if(RFcv$overall[2] > SVMcv$overall[2]) {
  bestModel <- RFmodelFit
  best <- "random forest"
} else {
  bestModel <- SVMmodelFit
  best <- "support vector machine"
}
```

The best model was the `r best` model.

# Make The Final Predictions using the test data set

``` {r predict}
pred <- predict(bestModel,test)
```
The final predictions are: `r pred`

Write the predictions to separate files, per instructions, to submit for grading
``` {r writesubmission}
# write best predictions to files for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(pred)
```


